{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zdravko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zdravko/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/zdravko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Comment ID</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>Is Reply</th>\n",
       "      <th>Author</th>\n",
       "      <th>AuthorID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13040</th>\n",
       "      <td>ðŸ‘†ðŸ‘†Congratulation you have been selected among ...</td>\n",
       "      <td>UgwZdOmybEXnE59gWDJ4AaABAg.9a5phea4RK-9a5pje4ibaj</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Telegram me@mrbeast453</td>\n",
       "      <td>UCpd3kplR9EdsvjbCAtp1JPw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13042</th>\n",
       "      <td>This real?</td>\n",
       "      <td>UgyplM0Mxw9MGOaTufB4AaABAg.9a5pQPe__Yg9a5r_cjVepc</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>LuchoXDD</td>\n",
       "      <td>UCu7MB0zqeYeC1nbqwJpT9sQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13043</th>\n",
       "      <td>Thanks for watching and commenting send a dire...</td>\n",
       "      <td>UgyplM0Mxw9MGOaTufB4AaABAg.9a5pQPe__Yg9a5pfEbP4cd</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>WhatsAppðŸ‘‰+â‘ â‘¦â‘ â‘£â‘¤â‘§â‘¦â‘¤â‘¥â‘¦â‘¦</td>\n",
       "      <td>UCLlW4UJkWDqW9Ht4u8LRztw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13044</th>\n",
       "      <td>ðŸ†™Congratulations! You've won the iPhone 13 pro...</td>\n",
       "      <td>UgyplM0Mxw9MGOaTufB4AaABAg.9a5pQPe__Yg9a5pROf4bhJ</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Telegram Me ðŸ”—IamMattJones</td>\n",
       "      <td>UClLJBDGFBNEvfuqO7mnISDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13046</th>\n",
       "      <td>Hay bro</td>\n",
       "      <td>Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a6Iw7cAVSq</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Alpixi</td>\n",
       "      <td>UCyVbB_mQsLtyDsXx7hWjqyg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13047</th>\n",
       "      <td>Soy un boi este no es mi cueta</td>\n",
       "      <td>Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a5r_EO0WI3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Alpixi</td>\n",
       "      <td>UCyVbB_mQsLtyDsXx7hWjqyg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13048</th>\n",
       "      <td>envÃ­ame un mensaje ahora mismo</td>\n",
       "      <td>Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a5py8YfWe0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>WhatsAppðŸ‘‰+â‘ â‘¦â‘ â‘£â‘¤â‘§â‘¦â‘¤â‘¥â‘¦â‘¦</td>\n",
       "      <td>UCblmGPQJ1_O_gBMmGRiVM9Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13049</th>\n",
       "      <td>Soy de Ecuador</td>\n",
       "      <td>Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a5pl-mh0ZR</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Alpixi</td>\n",
       "      <td>UCyVbB_mQsLtyDsXx7hWjqyg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13050</th>\n",
       "      <td>Sabe a lar ingles</td>\n",
       "      <td>Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a5ph9T8lwS</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Alpixi</td>\n",
       "      <td>UCyVbB_mQsLtyDsXx7hWjqyg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13052</th>\n",
       "      <td>No hablo ingles no entiendo el comentario</td>\n",
       "      <td>UgwjMvpoD-z6UFvKd7R4AaABAg.9a5pGzGQPfP9a5qsh45tU2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Super drenx</td>\n",
       "      <td>UCde_fu2FRIZLQwrtzlD69sg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Comments  \\\n",
       "13040  ðŸ‘†ðŸ‘†Congratulation you have been selected among ...   \n",
       "13042                                         This real?   \n",
       "13043  Thanks for watching and commenting send a dire...   \n",
       "13044  ðŸ†™Congratulations! You've won the iPhone 13 pro...   \n",
       "13046                                            Hay bro   \n",
       "13047                     Soy un boi este no es mi cueta   \n",
       "13048                     envÃ­ame un mensaje ahora mismo   \n",
       "13049                                     Soy de Ecuador   \n",
       "13050                                  Sabe a lar ingles   \n",
       "13052          No hablo ingles no entiendo el comentario   \n",
       "\n",
       "                                              Comment ID  Reply Count  \\\n",
       "13040  UgwZdOmybEXnE59gWDJ4AaABAg.9a5phea4RK-9a5pje4ibaj            0   \n",
       "13042  UgyplM0Mxw9MGOaTufB4AaABAg.9a5pQPe__Yg9a5r_cjVepc            0   \n",
       "13043  UgyplM0Mxw9MGOaTufB4AaABAg.9a5pQPe__Yg9a5pfEbP4cd            0   \n",
       "13044  UgyplM0Mxw9MGOaTufB4AaABAg.9a5pQPe__Yg9a5pROf4bhJ            0   \n",
       "13046  Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a6Iw7cAVSq            0   \n",
       "13047  Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a5r_EO0WI3            0   \n",
       "13048  Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a5py8YfWe0            0   \n",
       "13049  Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a5pl-mh0ZR            0   \n",
       "13050  Ugx9BcyffyeUY04bSZV4AaABAg.9a5pPwGaBsn9a5ph9T8lwS            0   \n",
       "13052  UgwjMvpoD-z6UFvKd7R4AaABAg.9a5pGzGQPfP9a5qsh45tU2            0   \n",
       "\n",
       "       Is Reply                     Author                  AuthorID  \n",
       "13040      True     Telegram me@mrbeast453  UCpd3kplR9EdsvjbCAtp1JPw  \n",
       "13042      True                   LuchoXDD  UCu7MB0zqeYeC1nbqwJpT9sQ  \n",
       "13043      True      WhatsAppðŸ‘‰+â‘ â‘¦â‘ â‘£â‘¤â‘§â‘¦â‘¤â‘¥â‘¦â‘¦  UCLlW4UJkWDqW9Ht4u8LRztw  \n",
       "13044      True  Telegram Me ðŸ”—IamMattJones  UClLJBDGFBNEvfuqO7mnISDA  \n",
       "13046      True                     Alpixi  UCyVbB_mQsLtyDsXx7hWjqyg  \n",
       "13047      True                     Alpixi  UCyVbB_mQsLtyDsXx7hWjqyg  \n",
       "13048      True      WhatsAppðŸ‘‰+â‘ â‘¦â‘ â‘£â‘¤â‘§â‘¦â‘¤â‘¥â‘¦â‘¦  UCblmGPQJ1_O_gBMmGRiVM9Q  \n",
       "13049      True                     Alpixi  UCyVbB_mQsLtyDsXx7hWjqyg  \n",
       "13050      True                     Alpixi  UCyVbB_mQsLtyDsXx7hWjqyg  \n",
       "13052      True                Super drenx  UCde_fu2FRIZLQwrtzlD69sg  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('comments.csv', sep=',', header=0)\n",
    "data.head()\n",
    "\n",
    "data[data['Is Reply'] == True][6600:6610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scamChannels = ['UCwgMHGG8IDcYjjSXtYbE9rQ', 'UC8Oy99fOvCjHfbvTImDXpkg',\n",
    "'UCswOElw6g7pEeAA5Mu15p3Q', 'UCIknJ8HTOMLSfIAmtsm84vA', 'UCsXIMkerN0ofYQVIvNWr7Dg', 'UC5iFMKp-Tuf2RX8wTDIA00w',\n",
    "'UC4BTXtDeOzz85XMShFWY1VA', 'UCIIab0_13sTLxQY66QjpM-g', 'UCZIIrgNdsq0MPdbG6utOLmw', 'UCf3mY9fz0oesuFS4fUnhjsg',\n",
    "'UCii92DZYgGqYquT71D9cWSQ', 'UCSkoxrUobYERf5FI1F0KDTg', 'UCbdM2ysSVcPxHghp50tiPQw', 'UC1Wi-CaZ_u111EET3es-jzA',\n",
    "'UCJB7ZvEbo-xRZum_3Zmq9sw', 'UCe5DcGeti6adyFedf49MRuA', 'UCfOYoX3Cwcn0A8t32c3cM5g', 'UCIAYvaVP15Cel4NZ1ib_ADA',\n",
    "'UC8Oy99fOvCjHfbvTImDXpkg', 'UCgvlSYolCHq5JiiosIvM6lw', 'UCcxxhVKa9wVvHXscwMXC2gQ', 'UCs5NZ-lIbR_rvYDaKvBrVFw',\n",
    "'UCHCtAgNRSrcBMRqU5SfHplw', 'UC07gLBHCOrcOyE0bw8Lv8Jg', 'UCnOW6JnwbE46hKOPyDYxc_w', 'UCLlW4UJkWDqW9Ht4u8LRztw',\n",
    "'UCoPBjpeflrKudIDsvDEV0aw', 'UCcoKvgQgWLXosFP5giUeI4g', 'UChYGnlCGDagZT0F8GYhDY2w', 'UCapA77PmpW9dEZQxxJY7TIg',\n",
    "'UCfdoVOJ9krvIZReKoLWAelA', 'UCU5WWthBfCSYPt-YRNRZYPQ', 'UCDDbjiG_3PHqn4_8BRMr9yA', 'UCVnjpK8HoaeYGQCJUjtiq6g',\n",
    "'UCkJePyY8lKMvWSF9S-X_hpQ', 'UCcoKvgQgWLXosFP5giUeI4g', 'UCcoKvgQgWLXosFP5giUeI4g', 'UCwRfjTiJGrONLlNHFQ0mQ2A',\n",
    "'UCJoDuLaZaOmmTLk9IVst6OA', 'UCk4bBXoqPm8j2FjyStas7jQ', 'UC6M_UOk3D8ZG0J2HmuK6J7g', 'UCkJePyY8lKMvWSF9S-X_hpQ',\n",
    "'UCk4bBXoqPm8j2FjyStas7jQ', 'UCgHM7Mjvat503rAX9jlFf3w', 'UCzywEh2M-ReJTvlPh-jA1-w', 'UC1Wi-CaZ_u111EET3es-jzA',\n",
    "'UClmn1s3aSJcqsGoQG7ZV5rg', 'UCIAYvaVP15Cel4NZ1ib_ADA', 'UC8Oy99fOvCjHfbvTImDXpkg', 'UCcxxhVKa9wVvHXscwMXC2gQ', \n",
    "'UCHCtAgNRSrcBMRqU5SfHplw', 'UC07gLBHCOrcOyE0bw8Lv8Jg', 'UCHCtAgNRSrcBMRqU5SfHplw', 'UCk4bBXoqPm8j2FjyStas7jQ', \n",
    "'UClmXrUcfU0A3BWSCU1XouNg', 'UCDhBrg4uKxdc0u5mv2x9GAA', 'UCwgMHGG8IDcYjjSXtYbE9rQ', 'UChYGnlCGDagZT0F8GYhDY2w', \n",
    "'UCcoKvgQgWLXosFP5giUeI4g', 'UC4A5Ov8NNZ8K1Lq5cfcESUg', 'UC7qpkq26VEDSe6H3ZwTMWJg', 'UCgvlSYolCHq5JiiosIvM6lw',\n",
    "'UCk44Jx55IZkD57On49NFtiw', 'UCPDuYbsUJxNexaCBruG-jkw', 'UCOqxwnL0H8oCTHlBNO9uSGQ', 'UCnOW6JnwbE46hKOPyDYxc_w', \n",
    "'UCblmGPQJ1_O_gBMmGRiVM9Q', 'UC4IerKEP2BDFd2VVXLaeU7w', 'UCVnjpK8HoaeYGQCJUjtiq6g', 'UChoTld7IoAinX5E7nYTZJVg',\n",
    "'UCwgMHGG8IDcYjjSXtYbE9rQ', 'UC4IerKEP2BDFd2VVXLaeU7w', 'UCYbUqEvxVhVh71wBbrgvyGA', 'UCEmzN-PVaElRDmdlBFbMW1A', \n",
    "'UCNBOZg8v0cFe2RYvSgJ_K3A', 'UCpTKksadLBwHiFOdtxVq5Sg', 'UCkGvXvq42A152M8S5xOMbBw', 'UC4kPR2QLYHmURMMr5ERPAcA', \n",
    "'UCJ4a59zmaTKVThYBU-ZZdFA', 'UCV_Qh1S9gCCXm2XBQRy0v8Q', 'UCsXIMkerN0ofYQVIvNWr7Dg', 'UCgP6tTK8LrlhK7LMhExs6eQ', \n",
    "'UCRqHZqWr81nXW0Qr2MVcD-g', 'UCpTKksadLBwHiFOdtxVq5Sg', 'UC1Wi-CaZ_u111EET3es-jzA', 'UCfH5_gBNYMszkN-6P1SAC1Q', \n",
    "'UCfH5_gBNYMszkN-6P1SAC1Q', 'UC8Oy99fOvCjHfbvTImDXpkg', 'UCYGbbdOfUAtBOFoiifA4cpw', 'UCFSHMdKANs7Ee_xAfvfX7ug', \n",
    "'UChYuxekYJM1HXY6ThthPR8A', 'UCXLbuE7gWN6CVyliQHKP62w', 'UC1Wi-CaZ_u111EET3es-jzA', 'UCYGbbdOfUAtBOFoiifA4cpw',\n",
    "'UCpd3kplR9EdsvjbCAtp1JPw', 'UCOLzBAcpeCiue5PnlPSvczQ', 'UCpd3kplR9EdsvjbCAtp1JPw', 'UCMhUkjPmAi9xrbskMrdNPWA',\n",
    "'UCLlW4UJkWDqW9Ht4u8LRztw'\n",
    "]\n",
    "\n",
    "\n",
    "def isScam(id):\n",
    "\n",
    "    if  id in scamChannels:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "data['Is Scam'] = data['AuthorID'].apply(lambda x: isScam(x))\n",
    "\n",
    "train = data[0:30000].copy()[['Comments', 'Is Reply', 'Author', 'Is Scam']]\n",
    "test = data[30001:35001].copy()[['Comments', 'Is Reply', 'Author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22979\n",
       "1     7021\n",
       "Name: Is Scam, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train['Is Scam'].value_counts()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Word Count'] = train['Comments'].apply(lambda x: len(str(x).split()))\n",
    "print(train[train['Is Scam']==1]['Word Count'].mean())\n",
    "print(train[train['Is Scam']==0]['Word Count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING WORD-COUNT\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "train_words=train[train['Is Scam']==1]['Word Count']\n",
    "ax1.hist(train_words,color='red')\n",
    "ax1.set_title('Scam Comments')\n",
    "train_words=train[train['Is Scam']==0]['Word Count']\n",
    "ax2.hist(train_words,color='green')\n",
    "ax2.set_title('Non-scam Comments')\n",
    "fig.suptitle('Words per comment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = str(text).lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "train['Clean Comment'] = train['Comments'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Clean text tokens']=[nltk.word_tokenize(i) for i in train['Clean Comment']]\n",
    "model = Word2Vec(train['Clean text tokens'],min_count=1)     \n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['Clean Comment'], train['Is Scam'], test_size=0.4, shuffle=True)\n",
    "\n",
    "X_train_tok = [nltk.word_tokenize(i) for i in X_train]\n",
    "X_test_tok = [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing the new dataset\n",
    "test['Clean Comments'] = test['Comments'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
    "X_test=test['Clean Comments']\n",
    "#converting words to numerical data using tf-idf\n",
    "X_vector=tfidf_vectorizer.transform(X_test)\n",
    "#use the best model to predict 'target' value for the new dataset \n",
    "y_predict = lr_tfidf.predict(X_vector)      \n",
    "y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "\n",
    "test['predict_prob']= y_prob\n",
    "test['Is Scam']= y_predict\n",
    "final=test[['Clean Comments', 'predict_prob', 'Is Scam']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c77533330135c96b7017aeac60f40449c443bdc60ec2ebb36fe283a0e265c63d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
