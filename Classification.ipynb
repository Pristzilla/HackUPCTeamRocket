{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jbcse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jbcse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jbcse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Comment ID</th>\n",
       "      <th>Reply Count</th>\n",
       "      <th>Is Reply</th>\n",
       "      <th>Author</th>\n",
       "      <th>AuthorID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>ðŸ‘†ðŸ‘†ðŸ‘†\\nThanks for watching and leaving a comment...</td>\n",
       "      <td>Ugw2NYiIjCiRnr5mUpx4AaABAg.9aMePefk-wu9aMh9GrQhE0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Telegram@ðŸ‘‰ðŸ‘‰mrbeast770</td>\n",
       "      <td>UCwgMHGG8IDcYjjSXtYbE9rQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>Congratulations you have been selected amongst...</td>\n",
       "      <td>Ugw2NYiIjCiRnr5mUpx4AaABAg.9aMePefk-wu9aMefMoxJJh</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>TELEGRAM ðŸ‘‰ MRBEAST6688</td>\n",
       "      <td>UCOqxwnL0H8oCTHlBNO9uSGQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>ðŸ‘†ðŸ‘†ðŸ‘†\\nThanks for watching and leaving a comment...</td>\n",
       "      <td>UgzjDdzgbDNfewCcgDt4AaABAg.9aMeN7_EuE49aMhIHnRt_M</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Telegram@ðŸ‘‰ðŸ‘‰mrbeast770</td>\n",
       "      <td>UCwgMHGG8IDcYjjSXtYbE9rQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>Congratulations you have been selected amongst...</td>\n",
       "      <td>UgyTmxKtlsBG3GDOW1x4AaABAg.9aMdm3Zkln99aMeGjHL_3I</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>TELEGRAM ðŸ‘‰ MRBEAST6688</td>\n",
       "      <td>UCOqxwnL0H8oCTHlBNO9uSGQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>ðŸ‘†ðŸ‘†ðŸ‘†\\nThanks for watching and leaving a comment...</td>\n",
       "      <td>UgyXisXjRZrctz-CbRB4AaABAg.9aMdLQlECGh9aMhacrkx89</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Telegram@ðŸ‘‰ðŸ‘‰mrbeast770</td>\n",
       "      <td>UCwgMHGG8IDcYjjSXtYbE9rQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>ðŸ‘†ðŸ‘†ðŸ‘†\\nThanks for watching and leaving a comment...</td>\n",
       "      <td>Ugwqas8o6Pyp3EsSYI54AaABAg.9aMcvfgRaUA9aMhjlZooGS</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Telegram@ðŸ‘‰ðŸ‘‰mrbeast770</td>\n",
       "      <td>UCwgMHGG8IDcYjjSXtYbE9rQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>Congratulations you have been selected amongst...</td>\n",
       "      <td>Ugwqas8o6Pyp3EsSYI54AaABAg.9aMcvfgRaUA9aMd4F67uBs</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>TELEGRAM ðŸ‘‰ MRBEAST6688</td>\n",
       "      <td>UCOqxwnL0H8oCTHlBNO9uSGQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>Congratulations you have been selected amongst...</td>\n",
       "      <td>UgyvVKPc9cIMrSF0gXh4AaABAg.9aMc_lWesUB9aMcg3T2VBB</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>TELEGRAM ðŸ‘‰ MRBEAST6688</td>\n",
       "      <td>UCOqxwnL0H8oCTHlBNO9uSGQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>Inbox me to claim yours</td>\n",
       "      <td>UgwPBW2kYkJFMl9Mled4AaABAg.9aMcL7oR_0s9aMdFBjyt4n</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>TELEGRAM ðŸ‘‰ MRBEAST6688</td>\n",
       "      <td>UCOqxwnL0H8oCTHlBNO9uSGQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>Are you the real mr beast</td>\n",
       "      <td>UgwPBW2kYkJFMl9Mled4AaABAg.9aMcL7oR_0s9aMcxRAs7DY</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Yasanda Ransilu</td>\n",
       "      <td>UCMfTLkkdd0yFLVxkI1Dcxjw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comments  \\\n",
       "2537  ðŸ‘†ðŸ‘†ðŸ‘†\\nThanks for watching and leaving a comment...   \n",
       "2538  Congratulations you have been selected amongst...   \n",
       "2540  ðŸ‘†ðŸ‘†ðŸ‘†\\nThanks for watching and leaving a comment...   \n",
       "2542  Congratulations you have been selected amongst...   \n",
       "2544  ðŸ‘†ðŸ‘†ðŸ‘†\\nThanks for watching and leaving a comment...   \n",
       "2546  ðŸ‘†ðŸ‘†ðŸ‘†\\nThanks for watching and leaving a comment...   \n",
       "2547  Congratulations you have been selected amongst...   \n",
       "2549  Congratulations you have been selected amongst...   \n",
       "2551                            Inbox me to claim yours   \n",
       "2552                          Are you the real mr beast   \n",
       "\n",
       "                                             Comment ID  Reply Count  \\\n",
       "2537  Ugw2NYiIjCiRnr5mUpx4AaABAg.9aMePefk-wu9aMh9GrQhE0            0   \n",
       "2538  Ugw2NYiIjCiRnr5mUpx4AaABAg.9aMePefk-wu9aMefMoxJJh            0   \n",
       "2540  UgzjDdzgbDNfewCcgDt4AaABAg.9aMeN7_EuE49aMhIHnRt_M            0   \n",
       "2542  UgyTmxKtlsBG3GDOW1x4AaABAg.9aMdm3Zkln99aMeGjHL_3I            0   \n",
       "2544  UgyXisXjRZrctz-CbRB4AaABAg.9aMdLQlECGh9aMhacrkx89            0   \n",
       "2546  Ugwqas8o6Pyp3EsSYI54AaABAg.9aMcvfgRaUA9aMhjlZooGS            0   \n",
       "2547  Ugwqas8o6Pyp3EsSYI54AaABAg.9aMcvfgRaUA9aMd4F67uBs            0   \n",
       "2549  UgyvVKPc9cIMrSF0gXh4AaABAg.9aMc_lWesUB9aMcg3T2VBB            0   \n",
       "2551  UgwPBW2kYkJFMl9Mled4AaABAg.9aMcL7oR_0s9aMdFBjyt4n            0   \n",
       "2552  UgwPBW2kYkJFMl9Mled4AaABAg.9aMcL7oR_0s9aMcxRAs7DY            0   \n",
       "\n",
       "      Is Reply                  Author                  AuthorID  \n",
       "2537      True   Telegram@ðŸ‘‰ðŸ‘‰mrbeast770  UCwgMHGG8IDcYjjSXtYbE9rQ  \n",
       "2538      True  TELEGRAM ðŸ‘‰ MRBEAST6688  UCOqxwnL0H8oCTHlBNO9uSGQ  \n",
       "2540      True   Telegram@ðŸ‘‰ðŸ‘‰mrbeast770  UCwgMHGG8IDcYjjSXtYbE9rQ  \n",
       "2542      True  TELEGRAM ðŸ‘‰ MRBEAST6688  UCOqxwnL0H8oCTHlBNO9uSGQ  \n",
       "2544      True   Telegram@ðŸ‘‰ðŸ‘‰mrbeast770  UCwgMHGG8IDcYjjSXtYbE9rQ  \n",
       "2546      True   Telegram@ðŸ‘‰ðŸ‘‰mrbeast770  UCwgMHGG8IDcYjjSXtYbE9rQ  \n",
       "2547      True  TELEGRAM ðŸ‘‰ MRBEAST6688  UCOqxwnL0H8oCTHlBNO9uSGQ  \n",
       "2549      True  TELEGRAM ðŸ‘‰ MRBEAST6688  UCOqxwnL0H8oCTHlBNO9uSGQ  \n",
       "2551      True  TELEGRAM ðŸ‘‰ MRBEAST6688  UCOqxwnL0H8oCTHlBNO9uSGQ  \n",
       "2552      True         Yasanda Ransilu  UCMfTLkkdd0yFLVxkI1Dcxjw  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('comments.csv', sep=',', header=0)\n",
    "data.head()\n",
    "\n",
    "data[data['Is Reply'] == True][1300:1310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scamChannels = ['UCwgMHGG8IDcYjjSXtYbE9rQ', 'UCIAYvaVP15Cel4NZ1ib_ADA', 'UC8Oy99fOvCjHfbvTImDXpkg',\n",
    "'UCswOElw6g7pEeAA5Mu15p3Q', 'UCIknJ8HTOMLSfIAmtsm84vA', 'UCsXIMkerN0ofYQVIvNWr7Dg', 'UC5iFMKp-Tuf2RX8wTDIA00w',\n",
    "'UC4BTXtDeOzz85XMShFWY1VA', 'UCIIab0_13sTLxQY66QjpM-g', 'UCZIIrgNdsq0MPdbG6utOLmw', 'UCf3mY9fz0oesuFS4fUnhjsg',\n",
    "'UCii92DZYgGqYquT71D9cWSQ', 'UCSkoxrUobYERf5FI1F0KDTg', 'UCbdM2ysSVcPxHghp50tiPQw', 'UC1Wi-CaZ_u111EET3es-jzA',\n",
    "'UCJB7ZvEbo-xRZum_3Zmq9sw', 'UCe5DcGeti6adyFedf49MRuA', 'UCfOYoX3Cwcn0A8t32c3cM5g', 'UCIAYvaVP15Cel4NZ1ib_ADA',\n",
    "'UC8Oy99fOvCjHfbvTImDXpkg', 'UCgvlSYolCHq5JiiosIvM6lw', 'UCcxxhVKa9wVvHXscwMXC2gQ', 'UCs5NZ-lIbR_rvYDaKvBrVFw',\n",
    "'UCHCtAgNRSrcBMRqU5SfHplw', 'UC07gLBHCOrcOyE0bw8Lv8Jg', 'UCnOW6JnwbE46hKOPyDYxc_w', 'UCLlW4UJkWDqW9Ht4u8LRztw',\n",
    "'UCoPBjpeflrKudIDsvDEV0aw', 'UCcoKvgQgWLXosFP5giUeI4g', 'UChYGnlCGDagZT0F8GYhDY2w', 'UCapA77PmpW9dEZQxxJY7TIg',\n",
    "'UCfdoVOJ9krvIZReKoLWAelA', 'UCU5WWthBfCSYPt-YRNRZYPQ', 'UCDDbjiG_3PHqn4_8BRMr9yA', 'UCVnjpK8HoaeYGQCJUjtiq6g']\n",
    "\n",
    "\n",
    "def isScam(id):\n",
    "\n",
    "    if  id in scamChannels:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "data['Is Scam'] = data['AuthorID'].apply(lambda x: isScam(x))\n",
    "\n",
    "train = data[0:30000].copy()[['Comments', 'Is Reply', 'Author', 'Is Scam']]\n",
    "test = data[30001:35001].copy()[['Comments', 'Is Reply', 'Author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25821\n",
       "1     4179\n",
       "Name: Is Scam, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train['Is Scam'].value_counts()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Word Count'] = train['Comments'].apply(lambda x: len(str(x).split()))\n",
    "print(train[train['Is Scam']==1]['Word Count'].mean())\n",
    "print(train[train['Is Scam']==0]['Word Count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING WORD-COUNT\n",
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
    "train_words=train[train['Is Scam']==1]['Word Count']\n",
    "ax1.hist(train_words,color='red')\n",
    "ax1.set_title('Scam Comments')\n",
    "train_words=train[train['Is Scam']==0]['Word Count']\n",
    "ax2.hist(train_words,color='green')\n",
    "ax2.set_title('Non-scam Comments')\n",
    "fig.suptitle('Words per comment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = str(text).lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "train['Clean Comment'] = train['Comments'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Clean text tokens']=[nltk.word_tokenize(i) for i in train['Clean Comment']]\n",
    "model = Word2Vec(train['Clean text tokens'],min_count=1)     \n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['Clean Comment'], train['Is Scam'], test_size=0.4, shuffle=True)\n",
    "\n",
    "X_train_tok = [nltk.word_tokenize(i) for i in X_train]\n",
    "X_test_tok = [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#building Word2Vec model\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_test_vectors_w2v = modelw.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing the new dataset\n",
    "test['Clean Comments'] = test['Comments'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
    "X_test=test['Clean Comments']\n",
    "#converting words to numerical data using tf-idf\n",
    "X_vector=tfidf_vectorizer.transform(X_test)\n",
    "#use the best model to predict 'target' value for the new dataset \n",
    "y_predict = lr_tfidf.predict(X_vector)      \n",
    "y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "\n",
    "test['predict_prob']= y_prob\n",
    "test['Is Scam']= y_predict\n",
    "final=test[['Clean Comments', 'predict_prob', 'Is Scam']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c77533330135c96b7017aeac60f40449c443bdc60ec2ebb36fe283a0e265c63d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
